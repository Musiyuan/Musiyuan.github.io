---
title: "A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications"
collection: publications
category: preprints
permalink: /publication/2025-08-18-A-Comprehensive-Survey-of-Mixture-of-Experts-Algorithms,-Theory,-and-Applications
Abstract: 'This paper provides a comprehensive survey of Mixture-of-Experts (MoE) models, covering their design, algorithmic advancements, and applications. MoE models, which dynamically select and activate relevant sub-models, offer efficient solutions for handling complex, multimodal data.'
date: 2025-08-18
venue: 'aiXiv'
paperurl: 'https://arxiv.org/pdf/2503.07137'
---

This paper is the first to comprehensively summarize recent advancements in MoE, including fundamental design strategies, algorithmic innovations, and applications in key machine learning paradigms such as continual learning, meta-learning, and multi-task learning, with a focus on their applications in computer vision and natural language processing.